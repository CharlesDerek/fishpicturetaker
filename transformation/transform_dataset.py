import os
import sys
from os.path import join, isfile, splitext
import argparse
import re
from shutil import rmtree, copy2
from PIL import Image, ImageOps, ImageEnhance
from distutils.dir_util import copy_tree
import math
import numpy as np


def main():
    args = parse_args()
    datasets_path = args.datasets_path if args.datasets_path != None else os.environ['FISHPIC_DATASETS_PATH']
    derived_datasets_path = args.derived_datasets_path if args.derived_datasets_path != None else os.environ['DERIVED_DATASETS_PATH']
    output_dir = join(derived_datasets_path, "transformed_fish_dataset")
    training_dir = join(output_dir, "training")

    if args.delete_augmented_data:
        delete_augmented_data(training_dir)
        sys.exit()

    if args.refresh:
        print("Deleting:", output_dir)
        if os.path.exists(output_dir):
            rmtree(output_dir)

    interim_dir = join(output_dir, "interim")

    handpicked_scraped_fish_data_dir = join(datasets_path, 'scraped_fish_data')
    print("Ignored these files:", convert_files_to_jpg(handpicked_scraped_fish_data_dir, interim_dir))
    noisy_web_dir = join(derived_datasets_path, 'filtered_supported_species_noisy_web_images')
    print("Ignored these files:", convert_files_to_jpg(noisy_web_dir, interim_dir))

    copy_fish_gov_au_images(datasets_path, training_dir)
    copy_fb_images(datasets_path, interim_dir)

    validation_dir = join(output_dir, "validation")
    testing_dir = join(output_dir, "testing")
    class_names_to_training_file_paths = split_files(interim_dir, training_dir, validation_dir, testing_dir)

    if args.augment_data:
        print("Augmenting training data")
        for class_name, file_paths in class_names_to_training_file_paths.items():
            for file_path in file_paths:
                augment_image(file_path, training_dir, class_name)

    # Copy accross the video frames after to the training dir after the splitting so that they are not deleted.
    # The video frames are put only into the training dir because if there were split, they would introduce
    # data bias into the validation and test sets as video frames are very similar to one another.
    print("Copy across video frames")
    fish_video_frames_dir = join(derived_datasets_path, 'fish_video_frames')
    print("Ignored these files:", convert_files_to_jpg(fish_video_frames_dir, training_dir))

    # TODO: Add contributor files by checking if the class is supported, and then adding them to the test sets.

    print("Finished datset transformation.")

def parse_args():
    parser = argparse.ArgumentParser(description='Transform the fish datasets into a structure appropriate for training the model.')
    parser.add_argument('--datasets_path', help='the path for the fish datasets.', required=False, default=None)
    parser.add_argument('--derived_datasets_path', help='the path to output the derived datasets.', required=False, default=os.environ['DERIVED_DATASETS_PATH'])
    parser.add_argument('--refresh', help='refresh the transformed files by deleting them at the beginning.', action='store_true', required=False)
    parser.add_argument('--augment_data', help='create augmentation version of all of the training data', action='store_true', required=False)
    parser.add_argument('--delete_augmented_data', help='delete all augmented data and do nothing else.', action='store_true', required=False)
    return parser.parse_args()

def delete_augmented_data(training_dir):
    print("Deleting augmented data.")
    augmenation_suffixes = [
        #"darker",
        #"lighter",
        "mirror",
        "rotated_left",
        "rotated_right",
        "rotated_upside_down",
    ]
    for class_dir in os.listdir(training_dir):
        for file_name in os.listdir(join(training_dir, class_dir)):
            if any([x in file_name for x in augmenation_suffixes]):
                path = join(training_dir, class_dir, file_name) 
                os.remove(path)

def convert_files_to_jpg(img_dir, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    ignored_file_names = []
    for file_name in os.listdir(img_dir):
        path = join(img_dir, file_name) 
        if isfile(path):
            continue # Ignore files in the img_dir
        print(path)
        output_child_dir = join(output_dir, file_name)
        if not os.path.exists(output_child_dir):
            os.makedirs(output_child_dir)

        for image_file_name in os.listdir(path):
            # Ignore the data.json files generated by the scraping script.
            if image_file_name.endswith(".json"):
                continue
            input_path = join(path, image_file_name)
            if input_path.endswith(".jpg"):
                output_path = join(output_child_dir, image_file_name)
                if os.path.exists(output_path):
                    if os.path.getsize(output_path) > 0:
                        print("Skipping existing file:", input_path)
                    else:
                        print("Ignoring empty file:", input_path)
                    continue
                # Attribution: https://stackoverflow.com/a/123212/137996
                copy2(input_path, output_path)
            else:
                output_path = join(output_child_dir, splitext(image_file_name)[0] + ".jpg")
                if os.path.exists(output_path):
                    print("Skipping existing file:", input_path)
                    continue
                try:
                    im = Image.open(input_path)
                    im.save(output_path)
                    print("Converted file to:", output_path)
                except OSError:
                    ignored_file_names.append(input_path)
                except StopIteration:
                    ignored_file_names.append(input_path)
                if os.path.exists(output_path) and os.path.getsize(output_path) == 0:
                    os.remove(output_path)

    return ignored_file_names

def copy_fish_gov_au_images(datasets_path, training_dir):
    print("Copying fish.gov.au images")
    fish_gov_au_supported_fish = ["barramundi", "dusky_flathead", "golden_snapper_fingermark", "grey_mackerel king_threadfin", "luderick", "mulloway", "sand_whiting", "sea_mullet", "snapper", "spanish_mackerel", "spotted_mackerel", "stout_whiting", "tailor", "yellowfin_bream", "yellowfin_tuna", "yellowtail_kingfish"]

    fish_gov_path = join(datasets_path, "fish.gov.au")
    for dir_path in os.listdir(fish_gov_path):
        if dir_path in fish_gov_au_supported_fish:
            # Copy directly into the training dir because we don't want these images to validation or test images because some of the fish.gov.au images are similar.
            copy_tree(join(fish_gov_path, dir_path), join(training_dir, dir_path))

def split_files(interim_dir, training_dir, validation_dir, testing_dir):
    class_names_to_training_file_paths = {}

    for class_dir in sorted(os.listdir(interim_dir)):
        print("Splitting %s" % class_dir)
        class_dir_path = join(interim_dir, class_dir)
        class_files = os.listdir(class_dir_path)
        validation_percentage = 10
        testing_percentage = 10
        n_validation_files = max(1, math.ceil(len(class_files) * validation_percentage  / 100))
        n_testing_files = max(1, math.ceil(len(class_files) * testing_percentage  / 100))
        n_training_files = len(class_files) - n_validation_files - n_testing_files
        #print(n_training_files, n_validation_files, n_testing_files)
        if n_training_files <= 0:
            n_validation_files = 0
            n_training_files = 1
        class_file_paths = set([ join(interim_dir, class_dir, x) for x in class_files ])
        # Set the random's seed to a constant value so that images are split the same way everytime this script is executed.
        # The training set split will change the moment a set of images for a particular fish changes unfortunately.
        random_state = np.random.RandomState(42)
        # sort these files so that they are in a consistent order which is important.
        validation_file_paths = set(random_state.choice(sorted(class_file_paths), n_validation_files))
        class_file_paths = class_file_paths - validation_file_paths
        testing_file_paths = set(random_state.choice(sorted(class_file_paths), n_testing_files))
        class_file_paths = class_file_paths - testing_file_paths
        #print(len(class_file_paths), n_training_files)
        #assert(len(class_file_paths) == n_training_files)
        training_file_paths = class_file_paths
        # Refresh the folders it in case the list of files has changed. We can't have one file added to more than one set.
        copy_files(validation_file_paths, join(validation_dir, class_dir), refresh=True)
        copy_files(testing_file_paths, join(testing_dir, class_dir), refresh=True)
        copy_files(training_file_paths, join(training_dir, class_dir), refresh=True)

        class_names_to_training_file_paths[class_dir] = training_file_paths
    #rmtree(interim_dir)
    return class_names_to_training_file_paths

def copy_fb_images(datasets_path, interim_dir):
    print("Copying FB images")
    fb_path = join(datasets_path, "facebook")
    supported_class_names = get_supported_class_names()
    copied_class_names = []
    ignored_class_names = []
    for class_name in os.listdir(fb_path):
        if class_name in supported_class_names:
            file_paths = [join(fb_path, class_name, x) for x in os.listdir(join(fb_path, class_name))]
            copy_files(file_paths, join(interim_dir, class_name))
            copied_class_names.append(class_name)
        else:
            ignored_class_names.append(class_name)
    print("Copied FB classes:" + str(copied_class_names))
    print("Ignored FB classes:" + str(ignored_class_names))

def get_supported_class_names():
    with open("../data/supported_classnames.txt", "r") as f:
        return f.read().split("\n")

def copy_files(file_paths, destination, refresh=False):
    if refresh and os.path.exists(destination):
        rmtree(destination)

    if not os.path.isdir(destination):
        os.makedirs(destination)

    for file_path in file_paths:
        copy2(file_path, destination) 

def augment_image(file_path, training_dir, class_name):
    try:
        image = Image.open(file_path)
        base_name = splitext(os.path.basename(file_path))[0]
        augmented_image_path_format_str = join(training_dir, class_name, base_name + "_%s.jpg")

        mirror_image = ImageOps.mirror(image)
        mirror_image.save(augmented_image_path_format_str % "mirror")

        rotated_left_image = image.rotate(90)
        rotated_left_image.save(augmented_image_path_format_str % "rotated_left")

        rotated_upside_down_image = image.rotate(180)
        rotated_upside_down_image.save(augmented_image_path_format_str % "rotated_upside_down")

        rotated_right_image = image.rotate(-90)
        rotated_right_image.save(augmented_image_path_format_str % "rotated_right")

        enhancer = ImageEnhance.Brightness(image)

        darker_image = enhancer.enhance(0.5) # reduce brightness level by 50%
        darker_image.save(augmented_image_path_format_str % "darker")

        lighter_image = enhancer.enhance(1.5) # increase brightness level by 50%
        lighter_image.save(augmented_image_path_format_str % "lighter")
    except Exception as e:
        print("Error: %s" % e)

if __name__ == "__main__":
    main()
